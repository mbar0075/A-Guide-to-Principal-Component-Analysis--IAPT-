{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80f09de0",
   "metadata": {},
   "source": [
    "## Name: Matthias Bartolo                            Id: 0436103L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5c3438",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "# <div style=\"text-align: left\"><font size=\"+4\"> A Guide to Principal Component Analysis </font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfdcfdd",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "## Table of Contents:\n",
    "### (Click any of the following links, to be redirected to that section)\n",
    "### [0. Introduction](#intro)\n",
    "### [1. Loading the Data](#loadData)\n",
    "### [2. Dataset Feature Selection](#featureSelection)\n",
    "### [3. Dealing with Discrete Data](#discreteData)\n",
    "### [4. Filtered Dataset Visualisations](#initialDataSetVisualisations)\n",
    "### [5. Normalizing Data](#normalizingData)\n",
    "### [6. Normalized Dataset Visualisations](#normalizedDataSetVisualisations)\n",
    "### [7. Understanding PCA - SVD Approach](#pcaSVD)\n",
    "### [8. Understanding PCA - Covariance Matrix Approach](#pcaCovariance)\n",
    "### [9. Comparisons Between Approaches](#approachCompare)\n",
    "### [10. Working out PCA on the Entire Dataset](#pcaEntire)\n",
    "### [11. PCA Visualisations](#pcaVisualisations)\n",
    "### [12. Conclusions and Limitations of PCA](#conclusion)\n",
    "### [13. References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087d9031",
   "metadata": {},
   "source": [
    "</br></br>\n",
    "### Packages Install (Please uncomment if you are receiving any errors):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cab29651",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install plotly\n",
    "#!pip install scikit-learn\n",
    "#!pip install pandas\n",
    "#!pip install gensim\n",
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cf912e",
   "metadata": {},
   "source": [
    "### Packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "018bce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import gensim\n",
    "import collections\n",
    "import nltk\n",
    "from os.path import exists\n",
    "from sklearn.decomposition import PCA\n",
    "from numpy import linalg as LA\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a147dd",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "<a id='intro'></a>\n",
    "### 0. Introduction\n",
    "</br>\n",
    "\n",
    "If you are a student, who just recently completed a **Linear Algebra** or **AI Numerical Methods course**, you might be wondering how these seemingly mathematical concepts relate to the real world. Incidentally, a noteworthy application of such concepts can be seen in the **Principal Component Analysis (PCA) Algorithm**, which is a widely used tool in various different fields such as Finance and Image Processing. Thus, the time and effort put into learning these mathematical ideas have not been in vain. Moreover, in this notebook we will be exploring how the techniques which you learned, relate together and form the PCA Algorithm.</br>\n",
    "\n",
    "**What is PCA?** </br>\n",
    "Principal Component Analysis (PCA) is an incredibly useful, and widely used multivariate algorithm in **Machine Learning**. Moreover, such algorithm is also extremely helpful in the analysis of huge datasets, whilst effectively undertaking **Dimensionality Reduction** and **Feature Selection**. Furthermore, PCA is used to ensure that data scientists may load and utilise large datasets on less powerful machines, which could not support the size of the full dataset. Additionally, PCA also provides cleaner data visualisation through the envisioning of the key data features in the full dataset, which hold the largest degree of information. [1-2]</br>\n",
    "\n",
    "Mathematically, PCA enables the conversion of linear continuous data into a new coordinate system, characterized by new axis **(Principal Components)** which are ordered in accordance with the features in the new coordinate system. This enables that the best principal components are plotted on different dimensional graphs, thus presenting a satisfactory visualisation of a large dataset. Unfortunately, such method may have some minimal data reduction, however visualising an n dimensional feature dataset on a 3D plot is quite a benefit. The PCA's main characteristics of decreasing the dimensionality of data, whilst retaining salient information, lead to it being the most effectively ranked data analysis and machine learning technique [1-2].</br></br>\n",
    "\n",
    "\n",
    "**Brief History of PCA:** </br>\n",
    "- PCA has a long, and illustrious history that goes back more than a century. The algorithm was pioneered by Karl Pearson, who in 1901 launched this system with the aim of undertaking data analysis and dimensionality reduction. The current PCA's design was first pioneered by Harold Hotelling in the 1930s, who led the way for the method to truly take shape. Hotelling was instrumental in formulating the concept of variance maximization, and the use of orthogonal projections to find the Principal Components. [1-2]\n",
    "- Further improvements to the PCA algorithm were developed in the 1960s, in part due to the emergence of Singular Value Decomposition (SVD), which offered an alternate method for calculating the eigenvalues and vectors, necessary to perform the PCA algorithm. The growing adoption of PCA at that time was largely triggered by the need for dimensionality reduction and the widespread growth of computers. Consequently, the method gained a lot of popularity in the 1970s and later on when data scientists and researchers fully comprehended the effectiveness of such technique in dealing with enormous and complex datasets which were becoming more and more prevalent in industries such as banking, engineering, and medicine. [1-2]\n",
    "\n",
    "\n",
    "</br></br>\n",
    "\n",
    "**Citations in this Section:** </br>\n",
    "\n",
    "[1] S. Mishra et al., \"Multivariate Statistical Data Analysis-Principal Component Analysis,\" Int. J. Livest. Res., vol. 1, pp. 1-6, 2017. [Online]. Available: https://www.researchgate.net/publication/316652806_Principal_Component_Analysis. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "\n",
    "[2] D. Li and S. Liu, \"4.2.3.1 Principal Component Analysis,\" in Water Quality Monitoring and Management: Basis, Technology and Case Studies, 1st ed., S. K. Gupta and R. Kumar, Eds. Amsterdam, Netherlands: Elsevier, 2019. [Online]. Available: https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/principal-component-analysis. [Accessed: 18-Apr-2023].\n",
    "\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ff344a",
   "metadata": {},
   "source": [
    "</br></br>\n",
    "\n",
    "<a id='loadData'></a>\n",
    "### 1. Loading the Data\n",
    "</br>\n",
    "\n",
    "A key step before the initiation of the PCA Algorithm, entails the selection of a relevant dataset which will be analysed by such algorithm. The designed implementation enables students interacting with the notebook, the choice to select any of the default datasets, and explore how the PCA algorithm will function on such datasets.  Students are also given the option to load their preferred dataset. In addition, the selected default datasets are characterised by different attributes, so as to allow students in carrying out different experiments, and to facilitate a comparative analysis of the results obtained through varying the datasets. **Additionally, students are also highly encouraged, before commencing the Principal Component Analysis, to thoroughly analyse and understand the dataset's properties and qualities.** </br></br>\n",
    "\n",
    "**The following are the default datasets (Obtained from [3-9]):**\n",
    "1. **country_wise_latest.csv** - This dataset has a small Size, a large number of Features, and a few numbers of Discrete Columns.\n",
    "2. **diabetes.csv** - This dataset has a small Size, a small number of Features, and no Discrete Columns.\n",
    "3. **FIFA - 2014.csv** - This dataset has a small Size, a small number of Features, and one Discrete Column.\n",
    "4. **IRIS.csv** - This dataset has a small Size, a large number of Features, and one Discrete Column.\n",
    "5. **Salary_Dataset_with_Extra_Features.csv** - This dataset has a large Size, a small number of Features, and a reasonable number of Discrete Columns.\n",
    "6. **spotify.csv** - This dataset has a large Size, a large number of Features, and a reasonable number of Discrete Columns.\n",
    "7. **wine-quality-white-and-red.csv** - This dataset has a large Size, a large number of Features, and one Discrete Column.\n",
    "\n",
    "</br>\n",
    "\n",
    "**In the code cell below, Students are presented with a Menu, either to load a default dataset or a preferred dataset of their choice.**\n",
    "\n",
    "</br></br>\n",
    "\n",
    "**Citations in this Section:** </br>\n",
    "\n",
    "\n",
    "[3] DEVAKUMAR K. P., \"COVID-19 Dataset\", Kaggle, 2020. [Online]. Available: https://www.kaggle.com/datasets/imdevskp/corona-virus-report?select=country_wise_latest.csv. [Accessed: 18-Apr-2023].</br>\n",
    " \n",
    "[4] UCI MACHINE LEARNING, \"Pima Indians Diabetes Database\", Kaggle, 2016. [Online]. Available: https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[5] S. BANERJEE, \"FIFA - Football World Cup Dataset\", Kaggle, 2022. [Online]. Available: https://www.kaggle.com/datasets/iamsouravbanerjee/fifa-football-world-cup-dataset?select=FIFA+-+2014.csv. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[6] MATHNERD, \"Iris Flower Dataset\", Kaggle, 2018. [Online]. Available: https://www.kaggle.com/datasets/arshid/iris-flower-dataset. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[7] S. BANERJEE, \"Software Industry Salary Dataset - 2022\", Kaggle, 2022. [Online]. Available: https://www.kaggle.com/datasets/iamsouravbanerjee/software-professional-salaries-2022. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[8] R. Holbrook and A. Cook, \"Principal Component Analysis, spotify.csv\", Kaggle. [Online]. Available: https://www.kaggle.com/code/ryanholbrook/principal-component-analysis/data?select=spotify.csv. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[9] RUTHGN, \"Wine Quality Data Set (Red & White Wine)\", Kaggle, 2022. [Online]. Available: https://www.kaggle.com/datasets/ruthgn/wine-quality-data-set-red-white-wine. [Accessed: 18-Apr-2023].</br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f82fd026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m \n",
      "Choosing From the following Options: \u001b[0m \n",
      "1.Load a Default Dataset\n",
      "2.Choose a Requested Dataset\n",
      "1\n",
      "\u001b[1m \n",
      "Choose from the Following Default Datasets: \u001b[0m\n",
      "country_wise_latest.csv\n",
      "diabetes.csv\n",
      "FIFA - 2014.csv\n",
      "IRIS.csv\n",
      "Salary_Dataset_with_Extra_Features.csv\n",
      "spotify.csv\n",
      "wine-quality-white-and-red.csv\n",
      "\n",
      "Please Input File Name:\n",
      "Salary_Dataset_with_Extra_Features.csv\n"
     ]
    }
   ],
   "source": [
    "#Giving the user the option to choose either a default dataset, or to enter his/her own dataset path\n",
    "filename=None\n",
    "path=\"\"\n",
    "#Looping until the user enters a valid choice\n",
    "while True:\n",
    "    #Displaying Menu\n",
    "    print(\"\\033[1m \\nChoosing From the following Options: \\033[0m \\n1.Load a Default Dataset\\n2.Choose a Requested Dataset\")\n",
    "    choice=int(input())\n",
    "    if(choice==1):\n",
    "        #Showing a list of default datasets to the user, and awaiting valid user choice\n",
    "        print(\"\\033[1m \\nChoose from the Following Default Datasets: \\033[0m\")\n",
    "        validDataset=[]\n",
    "        for dataset in os.listdir(\"Datasets\"):\n",
    "            validDataset.append(dataset)\n",
    "            print(dataset)\n",
    "            #Looping until user, enters a valid File Name\n",
    "        while filename not in validDataset:\n",
    "            print(\"\\nPlease Input File Name:\")\n",
    "            filename=input()\n",
    "        #Constructing File Path\n",
    "        path=\"Datasets/\"+filename\n",
    "        break\n",
    "        \n",
    "    elif(choice==2):\n",
    "        #Giving the user the option to load a preferred dataset\n",
    "        print(\"\\nPlease Input Requested File Path (Make sure that the file you wish to load is in the current directory, and is of the .csv type):\")\n",
    "        filename=input()\n",
    "        path=filename\n",
    "        #Error Checking whether dataset exists\n",
    "        if(exists(path)):\n",
    "            break\n",
    "        else:\n",
    "            print(\"\\033[91m \\n\\nError: Requested File Not Found \\033[0m\")\n",
    "            \n",
    "#Loading the data from a specified path, and storing csv contents in a dataframe, with correct Error Handling\n",
    "if(exists(path)):\n",
    "    dataframe = pd.read_csv(path)\n",
    "    #Error Checking whether dataframe has the relevant number of columns\n",
    "    if(len(dataframe.columns)<3):\n",
    "        print(\"\\033[91m \\n\\nWarning: The Dataset has less than 3 features/columns, and would present Errors later on in forthcoming sections. Please make sure to load a dataset with at least 3 features/columns \\033[0m\")\n",
    "else:\n",
    "    print(\"\\033[91m \\n\\nError: Requested File Not Found \\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bcbf12",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "### The following is the Requested Dataset loaded in a Pandas Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb27738e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5f850a",
   "metadata": {},
   "source": [
    "</br></br>\n",
    "\n",
    "<a id='featureSelection'></a>\n",
    "### 2. Dataset Feature Selection\n",
    "</br>\n",
    "\n",
    "Another key step when performing a data analysis or a machine learning study, pertains to observing the type and number of different **Genes/Features**, which the dataset possesses. This step is highly critical, as sometimes processing a huge number of features in the dataset may cause memory allocation issues or prolong the processing time of algorithms.  \n",
    "\n",
    "**Please note that in case less than three columns are chosen, the first three columns will be added to the filtered dataset. This is done, to ensure that the filtered dataset, would have enough  features for visualisation in the upcoming sections.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30786f50",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "### Displaying the number of Genes/Features in the Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2007b667",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The \\033[1m\",filename,\"\\033[0m dataset currently has \\033[1m\",len(dataframe.columns),\" different Genes/Features.\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f101074",
   "metadata": {},
   "source": [
    "</br> \n",
    "\n",
    "**In the code cell below, Students are presented all the dataset **Features/Columns** one by one, and they are given the option, to continue processing with such feature, or to discard it.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9559eba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe which will hold the selected features\n",
    "filteredDataframe=dataframe\n",
    "#Displaying Menu to the User, so that the user will choose which features to keep\n",
    "print(\"\\033[1mChoose whether to keep the following Genes/Features, to work with: \\033[0m \")\n",
    "for colNum,column in enumerate(dataframe.columns):\n",
    "    print(\"Do you wish to keep: \",colNum+1,\"\\b.\",column,\"? \\033[1m(Enter 'Y' to accept, and 'N' to decline)\\033[0m\")\n",
    "    choice=input()\n",
    "    if(choice!=\"Y\"):\n",
    "        filteredDataframe=filteredDataframe.drop(columns=[column])\n",
    "\n",
    "#Looping until size is smaller than 3 and adding the first 3 columns to the dataframe (Error Checking)\n",
    "count=0\n",
    "while(len(filteredDataframe.columns)<3):\n",
    "    filteredDataframe.insert(count,dataframe.columns[count],dataframe.iloc[:,0])\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90d6617",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "### The following is the Filtered Dataset with the Requested Genes/Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de03d1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(filteredDataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1310834f",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "### Displaying the number of Genes/Features in the Filtered Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cba84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The \\033[1m\",filename,\"\\033[0m dataset currently has \\033[1m\",len(filteredDataframe.columns),\" different Genes/Features \\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba29cf0",
   "metadata": {},
   "source": [
    "</br></br>\n",
    "\n",
    "<a id='discreteData'></a>\n",
    "### 3. Dealing with Discrete Data\n",
    "</br>\n",
    "As previously mentioned, PCA is designed to be utilised on continuous data [10]. This is a cardinal feature and dictates the need to transform discrete data into continuous data before using PCA on a dataset. This is critical, as discrete data lacks a continuous range of values and cannot be represented in the same way as continuous data for this cause.\n",
    "\n",
    "</br>\n",
    "\n",
    "**Types of Data:**\n",
    "- **Continous Data** - This type of data refers to values which belong to a set, and data can take any value between a bounded and unbounded interval. (For example a Worker's Pay.)\n",
    "- **Discrete Data** - This type of data refers to values which belong to a set, and every data value needs to be disctint. (For example a Worker's Name.)\n",
    "\n",
    "</br>\n",
    "\n",
    "There are various ways how discrete data can be **Transformed/Encoded** to continuous data, in order to be examined by the PCA. \n",
    "</br></br>\n",
    "\n",
    "**The following are different types of Encoders, which can be used:**\n",
    "1. **One-Hot Encoding**\n",
    "2. **Label Encoding**\n",
    "3. **Ordinal Encoding (Similar to Label Encoding)**\n",
    "4. **Count Encoding**\n",
    "5. **Word Embeddings Model**\n",
    "\n",
    "</br></br>\n",
    "\n",
    "**Citations in this Section:** </br>\n",
    "\n",
    "[10] V. Karthik, \"PCA for categorical features\", Stack Overflow, Dec. 2016. [Online]. Available: https://stackoverflow.com/questions/40795141/pca-for-categorical-features#:~:text=PCA%20is%20designed%20for%20continuous,yes%2C%20you%20can%20use%20PCA. [Accessed: 18-Apr-2023].</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3fc80c",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "###  Observing the type of Data for each Column in the Filtered Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d65ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(filteredDataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004ecddf",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "###  3.1 One-Hot Encoding on the first Discrete Data Column \n",
    "</br>\n",
    "\n",
    "**What is One-Hot Encoding?** </br>\n",
    "One-hot Encoding is a data preparation technique used to transform discrete variables into a format that enables the examination by machine learning algorithms. Consequently, this encoding algorithm works by creating a binary vector for each possible category in the data. Additionally, each binary vector would have a value of 1 or 0 symbolising the presence or absence of each category respectively. [11-13]\n",
    "\n",
    "</br>\n",
    "\n",
    "**In the code cell below, One-Hot Encoding is applied to the first Discrete Data Column, via the pd.get_dummies function.**\n",
    "\n",
    "</br></br>\n",
    "\n",
    "**Citations in this Section:** </br>\n",
    "\n",
    "[11] Datagy. \"Pandas get_dummies (One-Hot Encoding) Explained,\" Datagy.io, Feb. 2021. [Online]. Available: https://datagy.io/pandas-get-dummies/. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[12] DataCamp. \"Dealing with Categorical Data\". DataCamp, 2021. [Online]. Available: https://www.datacamp.com/tutorial/categorical-data. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[13] B. Roy, \"All about Categorical Variable Encoding,\" Towards Data Science, Jul. 2, 2019. [Online]. Available: https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02. [Accessed: 18-Apr-2023].</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2552717",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looping through all the columns in the data frame and checking whether column has object type (i.e., contains discrete data),\n",
    "#if so, applying one hot encoding on the first discrete column, and exiting\n",
    "oneHotEncDataframe=pd.DataFrame()\n",
    "columnName=None\n",
    "for column in filteredDataframe.columns:\n",
    "    if filteredDataframe[column].dtype=='O':#O denotes type Object\n",
    "        oneHotEncDataframe=pd.get_dummies(filteredDataframe[column])\n",
    "        columnName=column\n",
    "        break\n",
    "#Showing one-hot encoding dataframe of first discrete column\n",
    "display(oneHotEncDataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a92ead",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "### Displaying the number of Genes/Features in the Filtered Dataset, and One-Hot Encoded first Discrete Data Column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb00b7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The Filtered\\033[1m\",filename,\"\\033[0m dataset has \\033[1m\",len(filteredDataframe.columns),\" different Genes/Features \\033[0m\")\n",
    "if(columnName is None):\n",
    "    print(\"\\033[1mThere are no Discrete Columns\\033[0m\")\n",
    "else:\n",
    "    print(\"Applying \\033[1mOne-Hot Encoding only on the\",columnName,\"column\\033[0m, results in the encoded data to have \\033[1m\",len(oneHotEncDataframe.columns),\" different Genes/Features \\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa44e2c0",
   "metadata": {},
   "source": [
    "</br>\n",
    "As can be seen from the above result, such encoding is quite explosive, as the number of different Genes/Features obtained after applying One-Hot encoding on a single column, will greatly increase the number of columns depending on the number of distinct features in each column. For an algorithm which aims to reduce dimensionality, such approach to turn discrete data into continuous  data is quite inefficient, notwithstanding the increase in memory and time complexity presented.\n",
    "\n",
    "</br>\n",
    "\n",
    "**In case you thought whether this binary vector can be transformed back to decimal. Note that such encoding algorithm exists and is known as Binary to Decimal Decoding. The aforementioned  algorithm effectively transforms the binary vector back into a decimal value, thus reducing the size of the Genes/Features to their original number [14]. Essentially such encoding would take relatively more time whilst achieving the same results as Label Encoding or Ordinal Encoding.**\n",
    "\n",
    "</br></br>\n",
    "\n",
    "**Citations in this Section:** </br>\n",
    "\n",
    "\n",
    "[14] T. Crosley, \"What is the binary to decimal decoder?\", Quora, May 8, 2018. [Online]. Available: https://www.quora.com/What-is-the-binary-to-decimal-decoder. [Accessed: 18-Apr-2023].</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c1ac04",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "### 3.2  Label Encoding\n",
    "</br>\n",
    "\n",
    "**What is Label Encoding?** </br>\n",
    "Label Encoding is another data preparation technique which facilitates the transformation of discrete variables into a format that is easily readable by machine learning algorithms. Such encoder works by giving each distinct category a unique numeric value or code [12,13,15]. For instance, taking the list of categories [“hat”,”apple”,”cap”] will be encoded as [3,1,2] (as numeric values).\n",
    "\n",
    "</br>\n",
    "\n",
    "**In the code cell below, Label Encoding is applied through the pd.factorise function, and the sort flag applied to True, so that there wouldn't be in the order which they appeared first.**\n",
    "\n",
    "</br></br>\n",
    "\n",
    "**Citations in this Section:** </br>\n",
    "\n",
    "[12] DataCamp. \"Dealing with Categorical Data\". DataCamp, 2021. [Online]. Available: https://www.datacamp.com/tutorial/categorical-data. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[13] B. Roy, \"All about Categorical Variable Encoding,\" Towards Data Science, Jul. 2, 2019. [Online]. Available: https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[15] Pandas. \"pandas.factorize()\". pandas 1.4.0 documentation, Jan. 07, 2022. [Online]. Available: https://pandas.pydata.org/docs/reference/api/pandas.factorize.html. [Accessed: 18-Apr-2023].</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1596cc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looping through every colum, and applying the pd.factorise function with \"sort=True\" on the discrete data\n",
    "labelEncDataframe=filteredDataframe.copy()\n",
    "for column in labelEncDataframe.columns:\n",
    "    if labelEncDataframe[column].dtype=='O':#O denotes type Object\n",
    "        labelEncDataframe[column] = pd.factorize(filteredDataframe[column], sort=True)[0]\n",
    "#Displaying Label Encoding DataFrame        \n",
    "display(labelEncDataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7024fa36",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "### 3.3  Ordinal Encoding\n",
    "</br>\n",
    "\n",
    "**What is Ordinal Encoding?** </br>\n",
    "A similar data preparation technique to Label Encoding is Ordinal Encoding. Such encoder works by giving each distinct category a unique numeric value or code, based on the order which the category appeared first [12,13,16]. For instance, taking the list of categories [\"hat\",\"apple\",\"cap\"] will be encoded as [1,2,3] (as numeric values, and encoded in the order which they appeared).\n",
    "\n",
    "</br>\n",
    "\n",
    "**In the code cell below, Ordinal Encoding is applied through the pd.factorise function, and the sort flag applied to False, so that the elements would be classified in the order which they appeared first.**\n",
    "\n",
    "</br></br>\n",
    "\n",
    "**Citations in this Section:** </br>\n",
    "\n",
    "[12] DataCamp. \"Dealing with Categorical Data\". DataCamp, 2021. [Online]. Available: https://www.datacamp.com/tutorial/categorical-data. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[13] B. Roy, \"All about Categorical Variable Encoding,\" Towards Data Science, Jul. 2, 2019. [Online]. Available: https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[16] J. Brownlee, \"One-Hot Encoding for Categorical Data,\" Machine Learning Mastery, Aug. 17, 2020. [Online]. Available: https://machinelearningmastery.com/one-hot-encoding-for-categorical-data/. [Accessed: 18-Apr-2023].</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5a99e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looping through every colum, and applying the pd.factorise function on the discrete data\n",
    "ordinalEncDataframe=filteredDataframe.copy()\n",
    "for column in ordinalEncDataframe.columns:\n",
    "    if ordinalEncDataframe[column].dtype=='O':#O denotes type Object\n",
    "        ordinalEncDataframe[column] = pd.factorize(filteredDataframe[column])[0]\n",
    "#Displaying Ordinal Encoding DataFrame        \n",
    "display(ordinalEncDataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2e713a",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "### 3.4  Count Encoding\n",
    "</br>\n",
    "\n",
    "**What is Count Encoding?** </br>\n",
    "Another data preparation technique is Count Encoding. This encoder works by encoding each distinct category, with the number of times such category appeared [12-13]. For instance, if the category \"hat\" appeared 5 times, then \"hat\" will be encoded by the number 5.\n",
    "\n",
    "</br>\n",
    "\n",
    "**In the code cell below, Count Encoding is applied through the .value_counts and .map function on the discrete data.**\n",
    "\n",
    "</br></br>\n",
    "\n",
    "**Citations in this Section:** </br>\n",
    "\n",
    "[12] DataCamp. \"Dealing with Categorical Data\". DataCamp, 2021. [Online]. Available: https://www.datacamp.com/tutorial/categorical-data. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[13] B. Roy, \"All about Categorical Variable Encoding,\" Towards Data Science, Jul. 2, 2019. [Online]. Available: https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02. [Accessed: 18-Apr-2023].</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540b46be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looping through every colum, and applying the .value_counts and .map functions on the discrete data\n",
    "countEncDataframe=filteredDataframe.copy()\n",
    "for column in countEncDataframe.columns:\n",
    "    if countEncDataframe[column].dtype=='O':#O denotes type Object\n",
    "        colValueFreq=countEncDataframe[column].value_counts(dropna=False)\n",
    "        countEncDataframe[column] =countEncDataframe[column].map(lambda x : colValueFreq[x])\n",
    "#Displaying Count Encoding DataFrame    \n",
    "display(countEncDataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345e9ba8",
   "metadata": {},
   "source": [
    "\n",
    "</br>\n",
    "\n",
    "### 3.5  Word Embeddings Model (utilising Word2vec)\n",
    "</br>\n",
    "\n",
    "**What is a Word Embeddings Model?** </br>\n",
    "A Word Embeddings Model is a type of natural language processing (NLP) model which depict words as numerical vectors in a high-dimensional space. This model works by first training a neural network on a large corpus of text data, in order to represent words as dense, low-dimensional vectors. Each component of the word vector represents a specific aspect or characteristic of the word, such as its semantic meaning, part of speech, or syntactic context [17]. Mainly, the developed artefact focuses on the use of Word2vec, which is one type of Word Embeddings Model [17].\n",
    "\n",
    "</br>\n",
    "\n",
    "**In the code cell below, the Word Embeddings Model (Word2vec) is applied on the discrete data. Please note that the process may take some time to complete. Additionally, the encoded data will be in the form of the vector mean of all the current words in the sentence.**\n",
    "\n",
    "</br></br>\n",
    "\n",
    "**Citations in this Section:** </br>\n",
    "\n",
    "[17] Vatsal, \"Word2Vec Explained\", Towards Data Science, Jul. 29, 2021. [Online]. Available: https://towardsdatascience.com/word2vec-explained-49c52b4ccb71. [Accessed: 18-Apr-2023].</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eed4435",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looping through every colum, and training the Word2Vec model on thge discrete data\n",
    "wordEncDataframe=filteredDataframe.copy()\n",
    "for column in wordEncDataframe.columns:\n",
    "    if wordEncDataframe[column].dtype=='O':#O denotes type Object\n",
    "        print(\"\\033[1mCompleting Column:\\033[0m\",column)\n",
    "        #Tokenising current Column\n",
    "        tokens=wordEncDataframe[column].apply(lambda x: word_tokenize(str(x).lower()))\n",
    "        #Feeding the model the tokens\n",
    "        #min count refers to the number of words to consider, a count of 1 means\n",
    "        #we are considering words with a count of 1\n",
    "        wordEmbeddingsModel=Word2Vec(tokens,min_count=1)\n",
    "        \n",
    "        #Retrieving the mean of the vector of all the current tokens in the sentence, and checking that token is not nan\n",
    "        wordEncDataframe[column] = wordEncDataframe[column].apply(lambda x: \n",
    "            np.mean([wordEmbeddingsModel.wv[token] for token in word_tokenize(str(x).lower())], axis=0).tolist()[0] \n",
    "            if str(x) != 'nan' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d33f624",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "###  Displaying Dataframe after applying Word Embeddings Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd00100",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(wordEncDataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec109a2d",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "###  Giving the User the Option to choose his/her preferred Encoding Technique:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a421ac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\033[1mChoose which Encoding technique to utilise:\\n1. Label Encoding\\n2. Ordinal Encoding\\n3. Count Encoding \\n4. Word Embeddings Model\\033[0m\")\n",
    "userChoice=int(input())\n",
    "filteredContinousData=labelEncDataframe \n",
    "if(userChoice==2):\n",
    "    filteredContinousData=ordinalEncDataframe \n",
    "elif(userChoice==3):\n",
    "    filteredContinousData=countEncDataframe \n",
    "elif(userChoice==4):\n",
    "    filteredContinousData=wordEncDataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a752f835",
   "metadata": {},
   "source": [
    "</br></br>\n",
    "\n",
    "<a id='initialDataSetVisualisations'></a>\n",
    "### 4. Filtered Dataset Visualisations\n",
    "</br>\n",
    "Visualisation is a useful tool, as it aids in the process of identifying data visual patterns and characteristics. It is a common fact that individuals find it simpler to spot patterns and trends when data is presented visually, rather than in numerical or written form. Unfortunately, not all features can be projected on screen, as visualised data is limited to three dimensions. Thus, individuals need to choose which features to visualise, from a high-dimensional dataset with many features.\n",
    "</br></br>\n",
    "\n",
    "**In the code cell below, Students are presented with the list of features in the dataset, and are given the option to choose either feature for the three Dimensional Variables, which will be Represented Visually.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad22d60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying list of features\n",
    "print(\"\\n\\033[1mChoose 3 Features to represent the Data in 2D and 3D \\033[0m\")\n",
    "for colNum,column in enumerate(filteredContinousData.columns):\n",
    "    print(colNum,\"\\b.\",column,)\n",
    "#List which holds the valid Column ranges\n",
    "validColumnRanges=range(0,len(filteredContinousData.columns))\n",
    "#Variable Initialisation to null\n",
    "x=None\n",
    "y=None\n",
    "z=None\n",
    "\n",
    "#Looping until valid column index is entered for every Dimensional variable\n",
    "while x not in validColumnRanges:\n",
    "    x=int(input(\"\\nPlease Input a Valid Column Index to represent the \\033[1mX axis\\033[0m attribute in the Plot:\"))\n",
    "    \n",
    "while y not in validColumnRanges:\n",
    "    y=int(input(\"\\nPlease Input a Valid Column Index to represent the \\033[1mY axis\\033[0m attribute in the Plot:\"))\n",
    "    \n",
    "while z not in validColumnRanges:\n",
    "    z=int(input(\"\\nPlease Input a Valid Column Index to represent the \\033[1mZ axis\\033[0m attribute in the Plot:\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acd899e",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "**Please note that the visualisation tools used are quite interactive and allow Students to zoom in or zoom out of the plots, enabling them to recognize certain data trends better. Additionally, note that the colour component does not show any relationship between the variables, but is used as a marker in order to compare graph axis between 2D and 3D plots. Furthermore, there are some cases where some graphs would need to rerun the code cell in order to appear, this is due to the plotly limit of 10 graphs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727b9eee",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "###  The following is the Filtered Dataset with the selected Features visualised in 3D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7094798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a 3D scatter plot, from the respective user inputted columns\n",
    "fig3D = go.Figure(data=go.Scatter3d(\n",
    "    x=filteredContinousData.iloc[:,x],\n",
    "    y=filteredContinousData.iloc[:,y],\n",
    "    z=filteredContinousData.iloc[:,z],\n",
    "    mode='markers',\n",
    "    marker=dict(color = filteredContinousData.iloc[:,x],line=dict(width=2000, color='DarkSlateGrey'))\n",
    "))\n",
    "\n",
    "#Setting the respective title preferences\n",
    "fig3D.update_layout(width=1000, height=1000, title = '3D Representation of Filtered Dataset from '+filename+':',\n",
    "                  font_color=\"blue\",font_family=\"verdana\",\n",
    "                  scene = dict(xaxis=dict(title=filteredContinousData.columns[x], titlefont_color='blue'),\n",
    "                               yaxis=dict(title=filteredContinousData.columns[y], titlefont_color='blue'),\n",
    "                               zaxis=dict(title=filteredContinousData.columns[z], titlefont_color='blue')))\n",
    "#Displaying plot\n",
    "fig3D.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa847adc",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "###  The following is the Filtered Dataset with the selected Features visualised in 2D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1914dd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a 2D scatter plot, from the respective user inputted columns\n",
    "fig2D = px.scatter(x=filteredContinousData.iloc[:,x] ,y=filteredContinousData.iloc[:,y])\n",
    "fig2D.update_layout(title = '2D Representation of Filtered Dataset from '+filename+':',\n",
    "                    xaxis_title=filteredContinousData.columns[x],\n",
    "                    yaxis_title=filteredContinousData.columns[y],\n",
    "                    font_color=\"blue\",font_family=\"verdana\",coloraxis_showscale=False)\n",
    "#Updating settings\n",
    "fig2D.update_traces(marker=dict(color=filteredContinousData.iloc[:,x],size =10,line=dict(width=2,color='DarkSlateGrey')),selector=dict(mode='markers'))\n",
    "#Displaying plot\n",
    "fig2D.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3696d27a",
   "metadata": {},
   "source": [
    "</br></br>\n",
    "\n",
    "<a id='normalizingData'></a>\n",
    "### 5. Normalizing Data\n",
    "</br>\n",
    "\n",
    "**What is Normalization?** </br>\n",
    "Normalization is the process of converting and scaling the numerical characteristics inside a dataset, with the aim of ensuring that the data is characterized by a uniform range and distribution. Normalization's primary objective is to guarantee that no feature dominates or has an excessively large impact on the model's performance. **This process is critical in the calculation of the PCA, since if given unnormalized data, the PCA algorithm will load on the high variance data** [18]. An example would be having two data variables, one having a value of 1 and the other having a value of 700, whereby the PCA algorithm will issue higher importance to the second value. The importance of Normalizing data is highly significant especially when one considers that the previous encoding techniques, such as Label Encoding or Ordinal Encoding will provide transformed variables with an uneven distribution. Normalization addresses this issue once it converts the data values into a uniform range.</br></br>\n",
    "\n",
    "This implementation focuses on utilising **Z-Score Normalization** or also known as **Standardization** [19], and such normalization technique can be constructed through the following formula: \n",
    "\n",
    "<div style=\"text-align: center\"><font size=\"+3\"> $norm=\\frac{(x - \\mu)}{σ}$ </font></div></br>\n",
    "\n",
    "<font size=\"+0.5\">\n",
    "    \n",
    "**where:**\n",
    "1. **x** - is the Original Value\n",
    "2. **$\\mu$** - is the Mean of the Data\n",
    "2. **σ** - is the Standard Deviation of the Data\n",
    "3. **norm** - is the Normalized Data\n",
    "</font>\n",
    "\n",
    "</br></br>\n",
    "\n",
    "**Citations in this Section:** </br>\n",
    "\n",
    "[18] Stack Exchange. \"Why do we need to normalize data before Principal Component Analysis (PCA)?\", Cross Validated, May 26, 2014. [Online]. Available: https://stats.stackexchange.com/questions/69157/why-do-we-need-to-normalize-data-before-principal-component-analysis-pca. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[19] R. Sharma. \"What is Normalization in Data Mining and How to Do It?\", UpGrad, Sep. 22, 2022. [Online]. Available: https://www.upgrad.com/blog/normalization-in-data-mining/#:~:text=Project%20Ideas%20%26%20Topics-,Z%2DScore%20Normalization,up%20to%20%2B3%20standard%20deviation. [Accessed: 18-Apr-2023].</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c38630",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "###  5.1 First, Calculating the Mean ($\\mu$) for each Column in the Dataframe\n",
    "\n",
    "**In the code cell below, Students are presented with the calculation of the mean for each colum through the .mean function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8af9c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the Mean for each Column through the .mean() function, and storing result in a list\n",
    "colMean=list(filteredContinousData.mean())\n",
    "#Displaying Mean for each column\n",
    "for colNum, column in enumerate(colMean):\n",
    "    print(\"\\033[1mColumn:\\033[0m \",colNum+1,\"\\t\\033[1mMean: \\033[0m\",column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434dc392",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "###  5.2 Second, Calculating the Standard Deviation (σ) for each Column in the Dataframe\n",
    "\n",
    "</br><font size=\"+1\">**Calculating Standard Deviation through the Formula:**</font></br></br>\n",
    "\n",
    "<div style=\"text-align: center\"><font size=\"+3\"> $σ=\\sqrt{\\frac{\\sum (x_i - \\mu)^2}{N}}$ </font></div></br>\n",
    "\n",
    "<font size=\"+0.5\">\n",
    "    \n",
    "**where:**\n",
    "1. **$x_i$** - is a Single Value from the Whole Data\n",
    "2. **$\\mu$** - is the Mean of the Data\n",
    "2. **N** - is the Size of the Data\n",
    "3. **σ** - is the Standard Deviation of the Data\n",
    "</font>\n",
    "\n",
    "**In the code cell below, Students are presented with the calculation of the mean for each colum through the .std function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cc5f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the Standard Deviation for each Column through the .std() function, and storing result in a list\n",
    "colStandardDev=list(filteredContinousData.std())\n",
    "#Displaying Standard Deviation for each column\n",
    "for colNum, column in enumerate(colStandardDev):\n",
    "    print(\"\\033[1mColumn: \\033[0m\",colNum+1,\"\\t\\033[1mStandard Deviation: \\033[0m\",column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8792561d",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "###  5.3 Finally, Utilising the Calculated Mean and Standard Deviation for each Column, to Normalize the Dataframe\n",
    "\n",
    "</br><font size=\"+1\">**Applying Formula:**</font></br></br>\n",
    "\n",
    "<div style=\"text-align: center\"><font size=\"+3\"> $norm=\\frac{(x - \\mu)}{σ}$ </font></div></br>\n",
    "\n",
    "<font size=\"+0.5\">\n",
    "    \n",
    "**where:**\n",
    "1. **x** - is the Original Value\n",
    "2. **$\\mu$** - is the Mean of the Data\n",
    "2. **σ** - is the Standard Deviation of the Data\n",
    "3. **norm** - is the Normalized Data\n",
    "</font>\n",
    "\n",
    "**In the code cell below, Students are presented with the construction of the Z-Score Normalization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934c3ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionary which will hold the normalized data\n",
    "normalizedData=dict()\n",
    "counter=0\n",
    "#Looping through all the columns in the Dataframe\n",
    "for column in filteredContinousData:\n",
    "    #Dictionary which will hold the normalized Column\n",
    "    normalizedColumn=dict()\n",
    "    rowCounter=0\n",
    "    #Looping through every row in the current column\n",
    "    for row in filteredContinousData[column]:\n",
    "        #Checking whether Standard Deviation is not 0, and if so normalizing current value\n",
    "        if colStandardDev[counter]!=0:\n",
    "            normalizedColumn[rowCounter]=(row-colMean[counter])/colStandardDev[counter]\n",
    "        else:#Standard Deviation is 0, therefore setting value to 0\n",
    "            normalizedColumn[rowCounter]=0\n",
    "        #Incrementing row counter\n",
    "        rowCounter+=1\n",
    "    #appending normalized column to normalized data\n",
    "    normalizedData[column]=normalizedColumn\n",
    "    counter+=1\n",
    "\n",
    "#Converting data into pandas Dataframe\n",
    "normalizedDF=pd.DataFrame.from_dict(normalizedData)\n",
    "#Changing nan to 0\n",
    "normalizedDF=normalizedDF.replace(np.nan,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995288d3",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "###  Creating a function, which applies the afformentioned techniques in succession, in order to Normalize a given Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e05735",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to Normalize a Dataframe\n",
    "def NormalizeDF(inputDF):\n",
    "    #Calculating the Mean for each Column through the .mean() function, and storing result in a list\n",
    "    colMean=list(inputDF.mean())\n",
    "    #Calculating the Standard Deviation for each Column through the .std() function, and storing result in a list\n",
    "    colStandardDev=list(inputDF.std())\n",
    "    #Dictionary which will hold the normalized data\n",
    "    normalizedData=dict()\n",
    "    counter=0\n",
    "    #Looping through all the columns in the DataSet\n",
    "    for column in inputDF:\n",
    "        #Dictionary which will hold the normalized Column\n",
    "        normalizedColumn=dict()\n",
    "        rowCounter=0\n",
    "        #Looping through every row in the current column\n",
    "        for row in inputDF[column]:\n",
    "            #Checking whether Standard Deviation is not 0, and if so normalizing current value\n",
    "            if colStandardDev[counter]!=0:\n",
    "                normalizedColumn[rowCounter]=(row-colMean[counter])/colStandardDev[counter]\n",
    "            else:#Standard Deviation is 0, therefore setting value to 0\n",
    "                normalizedColumn[rowCounter]=0\n",
    "            #Incrementing row counter\n",
    "            rowCounter+=1\n",
    "        #appending normalized column to normalized data\n",
    "        normalizedData[column]=normalizedColumn\n",
    "        counter+=1\n",
    "\n",
    "    #Converting data into pandas DataFrame\n",
    "    normalizedDF=pd.DataFrame.from_dict(normalizedData)\n",
    "    #Changing nan to 0\n",
    "    normalizedDF=normalizedDF.replace(np.nan,0)\n",
    "    #Returning normalized Dataframe\n",
    "    return normalizedDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339888bc",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "###  The following is the Normalized Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4778062",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(normalizedDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e835570",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "<a id='normalizedDataSetVisualisations'></a>\n",
    "### 6. Normalized Dataset Visualisations\n",
    "\n",
    "</br>\n",
    "\n",
    "**In the code sections below, the normalized dataset can be visualised in 3D and 2D respectively. Furthermore, one might note how in the following plots, the normalized data is plotted on a smaller range of values, when compared to the original plots in the section above. Additionally, one can also notice how in the normalized plots, the data values are centred  around zero.**\n",
    "\n",
    "**Please also note that the colour of the points in this graph may change slightly from the above graphs, as the data values are now centred  around zero and the normalized plot is plotted on a smaller range of values.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ab9635",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "###  The following is the Normalized Dataset visualised in 3D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8fd6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a 3D scatter plot, from the respective user inputted columns (columns are the same as the prvious section)\n",
    "fig3DNormalized = go.Figure(data=go.Scatter3d(\n",
    "    x=normalizedDF.iloc[:,x],\n",
    "    y=normalizedDF.iloc[:,y],\n",
    "    z=normalizedDF.iloc[:,z],\n",
    "    mode='markers',\n",
    "    marker=dict(color = normalizedDF.iloc[:,x],line=dict(width=2000, color='DarkSlateGrey'))\n",
    "))\n",
    "\n",
    "#Setting the respective title preferences\n",
    "fig3DNormalized.update_layout(width=1000, height=1000, title = '3D Representation of Normalized Dataset from '+filename+':',\n",
    "                  font_color=\"blue\",font_family=\"verdana\",\n",
    "                  scene = dict(xaxis=dict(title=normalizedDF.columns[x], titlefont_color='blue'),\n",
    "                               yaxis=dict(title=normalizedDF.columns[y], titlefont_color='blue'),\n",
    "                               zaxis=dict(title=normalizedDF.columns[z], titlefont_color='blue')))\n",
    "#Displaying plot\n",
    "fig3DNormalized.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8aae8e",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "###  The following is the Normalized Dataset visualised in 2D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35047068",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a 2D scatter plot, from the respective user inputted columns\n",
    "fig2DNormalized = px.scatter(x=normalizedDF.iloc[:,x] ,y=normalizedDF.iloc[:,y])\n",
    "fig2DNormalized.update_layout(title = '2D Representation of Normalized Dataset from '+filename+':',\n",
    "                    xaxis_title=normalizedDF.columns[x],\n",
    "                    yaxis_title=normalizedDF.columns[y],\n",
    "                    font_color=\"blue\",font_family=\"verdana\",coloraxis_showscale=False)\n",
    "#Updating settings\n",
    "fig2DNormalized.update_traces(marker=dict(color=normalizedDF.iloc[:,x],size =10,line=dict(width=2,color='DarkSlateGrey')),selector=dict(mode='markers'))\n",
    "#Displaying plot\n",
    "fig2DNormalized.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba84dc29",
   "metadata": {},
   "source": [
    "</br></br>\n",
    "\n",
    "<a id='pcaSVD'></a>\n",
    "### 7. Understanding PCA - SVD Approach\n",
    "\n",
    "</br>\n",
    "\n",
    "**What is Single Value Decomposition (SVD)?** </br>\n",
    "The first step in Calculation of PCA via SVD Approach, was undertaken through the utilisation of the Singular Value Decomposition (SVD), which is a decomposition method aimed at factorising a matrix of **m x n** size into three components. The resultant components include **U** and **$V^T$**, which are two orthonormal matrices, and Sigma (**$\\Sigma$**) which is a diagonal matrix containing the singular values of the original matrix. Additionally, the size/magnitude of each singular value signifies the importance in explaining the data [20]. For example, a singular value of 10 will have a higher importance than a singular value of 5. \n",
    "\n",
    "</br></br>\n",
    "\n",
    "**Citations in this Section:** </br>\n",
    "\n",
    "[20] M. E. Wall, A. Rechtsteiner, and L. M. Rocha, \"Singular Value Decomposition and Principal Component Analysis,\" in Learning from Data: Concepts, Theory, and Methods, vol. 2, Springer, Boston, MA, 2007, pp. 151-176, doi: 10.1007/0-306-47815-3_5. [Online]. Available: https://www.researchgate.net/publication/2167923_Singular_Value_Decomposition_and_Principal_Component_Analysis. [Accessed: 18-Apr-2023].</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c805217f",
   "metadata": {},
   "source": [
    "###  Taking a small subset of the entire dataset if dataset has a larger size than a respective threshold, and working out the PCA algorithm, via the SVD Approach.\n",
    "</br>\n",
    "\n",
    "**Note that the dataset is being reduced to a tenth of its size, whilst maintaining the number of columns, in order to aid the student to better understand the concept, and method of calculation, in case the dataset has a larger size than the respective threshold of 10000.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2b4206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking a subset of the dataframe, normalizing it, and converting it to a numpy array, if the dataframe has a size more than 10000 \n",
    "sizeThreshold=10000 \n",
    "smallerDF=NormalizeDF(filteredContinousData[:int(len(normalizedDF)/10)])\n",
    "#Checking for the size\n",
    "if(len(normalizedDF)<=sizeThreshold):\n",
    "    smallerDF=NormalizeDF(filteredContinousData)\n",
    "SmalledDFMatrix=smallerDF.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7076fe7",
   "metadata": {},
   "source": [
    "</br><font size=\"+1\">**SVD Decomposition of matrix A with size m x n results in:**</font></br></br>\n",
    "\n",
    "<div style=\"text-align: center\"><font size=\"+3\"> $A = U.\\Sigma.V^T$ </font></div></br>\n",
    "\n",
    "<font size=\"+0.5\">\n",
    "    \n",
    "**where:**\n",
    "1. **U** - is an Orthonormal Matrix of size **m x m**\n",
    "2. **$\\Sigma$** - is a Diagonal Matrix of size **m x n**\n",
    "2. **$V^T$** - is an Orthonormal Matrix of size **n x n**\n",
    "</font></br>\n",
    "\n",
    "**In the code cell below, Students are presented with the SVD Decomposition of the smaller dataset, done programmatically through the np.linalg.svd function. Note that the svd function presents the singular values only, and thus the matrix $\\Sigma$ needs to be calculated, from such values whilst filling the empty slots with zeros.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341fd5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the svd, via np.linalg.svd function\n",
    "U, s, VT = np.linalg.svd(SmalledDFMatrix)\n",
    "#Constructing Sigma Matrix\n",
    "Sigma = np.zeros((smallerDF.shape[0], smallerDF.shape[1]))\n",
    "Sigma[:smallerDF.shape[1], :smallerDF.shape[0]] = np.diag(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e17ff6f",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "###  The following is the U Matrix from the SVD Decompostion on the Small Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173d929f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(U)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7cbcb8",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "###  The following is the Sigma Matrix from the SVD Decomposition on the Small Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0565464b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(Sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3d49d3",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "###  The following is the $V^T$ Matrix from the SVD Decompostion on the Small Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8877f73a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(VT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76d8593",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "###  The second step in the Calculation of PCA via SVD Approach, involves multiplying the U matrix by the $\\Sigma$ matrix.\n",
    "</br>\n",
    "\n",
    "This is done as the multiplication of **$U.\\Sigma$** presents a matrix whose columns give the projections of the data points on each principal axis.\n",
    "\n",
    "**In the code cell below, Students are presented with the construction of the matrix containing the Principal axis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bd55ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiplying U by Sigma to obtain Principal Axis\n",
    "pcaSmallData1=U@Sigma\n",
    "pd.DataFrame(pcaSmallData1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f70e5e",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "###  Calculating the Variance Ratio for each Principal Component in order to determine which are the best Principal Components in explaining the variation in the data\n",
    "</br>\n",
    "\n",
    "The use of variance ratios in PCA is done, in order to calculate the percentage of the overall variance in the data that each principal component contributes to. Each principal component in the PCA algorithm captures a specific amount of data variation, thus we can determine the percentage of the overall variation that each component accounts for by computing the variance ratio. Additionally, through the variance ratio we are able to determine which principal components are crucial for explaining the variation in the data. [21]</br>\n",
    "\n",
    "**Therefore, we are able to lower the number of dimensions in the data whilst preserving a sizeable portion of the overall variance, ultimately simplifying the data.**</br>\n",
    "\n",
    "\n",
    "</br><font size=\"+1\">**The Variance Ratio is calculated through the following Formula (obtained from [21-22]):**</font></br></br></br>\n",
    "\n",
    "<div style=\"text-align: center\"><font size=\"+2.4\"> $Variance Ratio = \\frac{\\lambda_i}{\\Sigma \\lambda_i}$ </font></div></br>\n",
    "\n",
    "<font size=\"+0.5\">\n",
    "    \n",
    "**where:**\n",
    "1. **$\\lambda_i$** - is the sum of the squared distance for each of the principal component\n",
    "2. **$\\Sigma \\lambda_i$** - is the sum of all squared distances for all of the principal components\n",
    "</font></br>\n",
    "\n",
    "**In the code cell below, Students are presented with the calculation of the Variance Ratio for the current PCA configuration.**\n",
    "\n",
    "\n",
    "</br></br>\n",
    "\n",
    "**Citations in this Section:** </br>\n",
    "\n",
    "[21] I. T. Jolliffe and J. Cadima, \"Principal component analysis: a review and recent developments,\" in The Data Deluge: Can Libraries Cope with E-Science? Proceedings of a Conference Held at the Royal Society, London, UK, 4-5 November 2004, vol. 463, Royal Society Publishing, 2016, pp. 21-36. doi: 10.1098/rsta.2015.0202.[Online]. Available: https://royalsocietypublishing.org/doi/10.1098/rsta.2015.0202. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[22] K. Guillaumier, \"Linear Algebra in Data Science and PCA\"</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b7b24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an array to hold the square of each principal component\n",
    "square=[0]*len(pcaSmallData1[0])\n",
    "#Looping through all the principal components, and calculating the square for each component\n",
    "for row in range(len(pcaSmallData1)):\n",
    "    for col in range(len(pcaSmallData1[row])):\n",
    "        square[col]+=(pcaSmallData1[row][col])**2 \n",
    "#Calculating the total Square\n",
    "totalSquare=sum(square)\n",
    "#Calculating the Variance Ratio by dividing the square by the total square and multiplying by 100 to obtain a percentage\n",
    "svdVarianceRatio=(square/totalSquare)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb80b018",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "###  Visualising the Variance Ratio in the form of a Scree Plot\n",
    "</br>\n",
    "Through the use of the Scree Plot, which displays the percentage of variation explained by each primary component, we are able to calculate the number of components required to account for a specific percentage of the overall variance in the data [23].\n",
    "\n",
    "The following methods can be used to determine the optimal number of principal components to retain [23]:\n",
    "1. **Elbow Method** - This method of selection adopts to retain all the principal components prior to the curve plateau in the Scree Plot. Moreover, this method works by pinpointing the point on the Scree Plot where the curve plateaus, and then selecting the number of components before this point as the ideal number of components to maintain.\n",
    "2. **Kaiser Rule** - This method of selection selects to retain all the principal components with eigenvalues which have at least a value of 1.\n",
    "3. **Proportion of Variance Plot** - This method of selection chooses to retain all the principal components which represent a percentage (%) amount of the variance.\n",
    "\n",
    "\n",
    "\n",
    "</br></br>\n",
    "\n",
    "**Citations in this Section:** </br>\n",
    "\n",
    "[23] S. Mangale, \"Scree Plot,\" Medium, Aug. 28, 2020. [Online]. Available: https://sanchitamangale12.medium.com/scree-plot-733ed72c8608. [Accessed: 18-Apr-2023].</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261e67f4",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "###  Visual Representation of Scree Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d23858",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a Scree Plot to visualise the importance of each Principal Component\n",
    "figScreeSVD = go.Figure()\n",
    "#Creating Line Graph\n",
    "figScreeSVD.add_trace(go.Scatter(y=svdVarianceRatio))\n",
    "#Creating Bar Graph, with values on top rounded to 2 decimal place\n",
    "figScreeSVD.add_trace(go.Bar(marker_color=svdVarianceRatio,y=svdVarianceRatio, text=list(np.around(np.array(svdVarianceRatio),2)),textposition='outside'))\n",
    "#Updating layout\n",
    "figScreeSVD.update_layout(title = 'Scree Plot of PCA via SVD Approach of small dataset from '+filename+':',\n",
    "                    yaxis_title=\"Percentage of Explained Variance\",\n",
    "                    xaxis_title=\"Components\",\n",
    "                    font_color=\"blue\",font_family=\"verdana\", showlegend=False)\n",
    "#Displaying plot\n",
    "figScreeSVD.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32389e0e",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "###  Visual Representation of Principal Components\n",
    "\n",
    "</br>\n",
    "\n",
    "**In the code sections below, the Best Principal Components obtained will be plotted on different axis, in order to show the correlation between the data, after the reduction in dimensions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d063e3cc",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "###  Plotting the best three Principal Components which retain the Highest Variance in a 3D plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b154b655",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a 3D scatter plot, from the respective Principal Components\n",
    "fig3DSVD = go.Figure(data=go.Scatter3d(\n",
    "    x=pcaSmallData1.T[0],\n",
    "    y=pcaSmallData1.T[1],\n",
    "    z=pcaSmallData1.T[2],\n",
    "    mode='markers',\n",
    "    marker=dict(color = pcaSmallData1.T[0],line=dict(width=2000, color='DarkSlateGrey'))\n",
    "))\n",
    "\n",
    "#Setting the respective title preferences\n",
    "fig3DSVD.update_layout(width=1000, height=1000, title = '3D Representation PCA Data via SVD Approach of small dataset from '+filename+':',\n",
    "                  font_color=\"blue\",font_family=\"verdana\",\n",
    "                  scene = dict(xaxis=dict(title=\"Principal Component 1\", titlefont_color='blue'),\n",
    "                               yaxis=dict(title=\"Principal Component 2\", titlefont_color='blue'),\n",
    "                               zaxis=dict(title=\"Principal Component 3\", titlefont_color='blue')))\n",
    "#Displaying plot\n",
    "fig3DSVD.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fed6c4",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "###  Plotting the best two Principal Components which retain the Highest Variance in a 2D plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc12209",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a 2D scatter plot, from the respective Principal Components\n",
    "fig2DSVD = px.scatter(x=pcaSmallData1.T[0] ,y=pcaSmallData1.T[1])\n",
    "fig2DSVD.update_layout(title = '2D Representation of PCA Data via SVD Approach of small dataset from '+filename+':',\n",
    "                    xaxis_title=\"Principal Component 1\",\n",
    "                    yaxis_title=\"Principal Component 2\",\n",
    "                    font_color=\"blue\",font_family=\"verdana\",coloraxis_showscale=False)\n",
    "#Updating settings\n",
    "fig2DSVD.update_traces(marker=dict(color=pcaSmallData1.T[0],size =10,line=dict(width=2,color='DarkSlateGrey')),selector=dict(mode='markers'))\n",
    "#Displaying plot\n",
    "fig2DSVD.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad04875",
   "metadata": {},
   "source": [
    "</br></br>\n",
    "\n",
    "<a id='pcaCovariance'></a>\n",
    "### 8. Understanding PCA - Covariance Matrix Approach\n",
    "\n",
    "</br>\n",
    "\n",
    "**What is a Covariance Matrix?** </br>\n",
    "Another approach which can be used to calculate PCA, is the Covariance Matrix Method. Notably, the first step in the implementation of such approach pertain to the development of the Covariance Matrix or also known as the **Covariance Variance Matrix**. The aforementioned matrix is a **n x n** symmetric matrix which is used to show the covariance values between adjacent pairs of items in a dataset of n attributes. Additionally, the diagonal elements of such matrix represent the variance of each element. [24] \n",
    "\n",
    "</br><font size=\"+1\">**An example of a **3 x 3 Covariance Matrix** for a dataset containing 3 variable **x, y, z**:**</font></br></br></br>\n",
    "\n",
    "<div style=\"text-align: center\"><font size=\"+2.4\">$$\\begin{bmatrix} cov(x,x) & cov(x,y) & cov(x,z) \\\\ cov(y,x) & cov(y,y) & cov(y,z) \\\\ cov(z,x) & cov(z,y) & cov(z,z) \\end{bmatrix}$$ </font></div></br>\n",
    "\n",
    "\n",
    "</br><font size=\"+1\">**Another notation for the Covariance Matrix above is as follows:**</font></br></br></br>\n",
    "\n",
    "<div style=\"text-align: center\"><font size=\"+2.4\">$$\\begin{bmatrix} var(x) & cov(x,y) & cov(x,z) \\\\ cov(y,x) & var(y) & cov(y,z) \\\\ cov(z,x) & cov(z,y) & var(z) \\end{bmatrix}$$ </font></div></br></br></br>\n",
    "\n",
    "</br><font size=\"+1\">**Calculation of Covariance for two variables x and y can be facilitate through the following Formula:**</font></br></br></br>\n",
    "\n",
    "<div style=\"text-align: center\"><font size=\"+2.5\"> $cov(x,y)=\\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{N}$ </font></div></br>\n",
    "\n",
    "<font size=\"+0.5\">\n",
    "    \n",
    "**where:**\n",
    "1. **$\\bar{x}$** - is the mean value of the x attribute\n",
    "2. **$\\bar{y}$** - is the mean value of the y attribute\n",
    "3. **$x_i$** - is the current data value of the x attribute\n",
    "4. **$y_i$** - is the current data value of the y attribute\n",
    "5. **$\\bar{N}$** - is the number of data points\n",
    "</font></br>\n",
    "\n",
    "\n",
    "**Interpreting Covariance:** </br>\n",
    "Through Covariance we can determine the direction of the linear relationship between two attributes [25].\n",
    "- In case both variables increase or decrease simultaneously, then their Covariance is positive.\n",
    "- In case one variable increases whilst the other decreases simultaneously, then their Covariance is negative.\n",
    "\n",
    "\n",
    "</br></br>\n",
    "\n",
    "**Citations in this Section:** </br>\n",
    "\n",
    "[24] CUEMATH, \"Covariance Matrix\", CUEMATH. [Online]. Available: https://www.cuemath.com/algebra/covariance-matrix/. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[25] Minitab LLC, \"Interpret the key results for Covariance\", Minitab Support, 2022. [Online]. Available: https://support.minitab.com/en-us/minitab/21/help-and-how-to/statistics/basic-statistics/how-to/covariance/interpret-the-results/key-results/. [Accessed: 18-Apr-2023].</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5abfd4",
   "metadata": {},
   "source": [
    "###  Taking a small subset of the entire dataset if dataset has a larger size than a respective threshold, and working out the PCA algorithm, via the Covariance Approach.\n",
    "</br>\n",
    "\n",
    "**Note that the dataset is being reduced to a tenth of its size, whilst maintaining the number of columns, in order to aid the student to better understand the concept, and method of calculation, in case the dataset has a larger size than the respective threshold of 10000.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1abe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking a subset of the dataframe, normalizing it, and converting it to a numpy array, if the dataframe has a size more than 10000\n",
    "sizeThreshold=10000 \n",
    "smallerDF=NormalizeDF(filteredContinousData[:int(len(normalizedDF)/10)])\n",
    "#Checking for the size\n",
    "if(len(normalizedDF)<=sizeThreshold):\n",
    "    smallerDF=NormalizeDF(filteredContinousData)\n",
    "SmalledDFMatrix=smallerDF.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46591a1",
   "metadata": {},
   "source": [
    "###  Construction of Covariance Matrix\n",
    "</br>\n",
    "\n",
    "**In the code cell below, Students are presented with the construction of the smaller dataset's Covariance Matrix, done programmatically through the np.cov function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecb82b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating covariance matrix through np.cov function\n",
    "covMatrix=np.cov(SmalledDFMatrix.T)\n",
    "pd.DataFrame(covMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f8805c",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "###  Computing the Eigenvectors and Eigenvalues for the Covariance Matrix\n",
    "</br>\n",
    "\n",
    "**Why do we compute the Eigen Decomposition for the Covariance Matrix?** </br>\n",
    "\n",
    "The eigenvectors and eigenvalues of the Covariance Matrix are essentially the directions of the axis where there is the highest variance (most data), which we refer to as Principal Components. Furthermore, the variance held by each Principal Component is shown by the eigenvalues, which are essentially the coefficients associated to the eigenvectors [26]. Therefore, computing and sorting the eigenvectors and eigenvalues by the order of largest eigenvalue first, will provide the required Principal Components sorted by their importance.\n",
    "\n",
    "</br>\n",
    "\n",
    "\n",
    "**In the code cell below, Students are presented with the computation of the Eigen Decomposition of the Covariance Matrix through the .eig function, and sorting the eigenvalues and eigenvectors by the order of largest eigenvalue first.**\n",
    "\n",
    "</br></br>\n",
    "\n",
    "**Citations in this Section:** </br>\n",
    "\n",
    "[26] Z. Jaadi, \"A Step-by-Step Explanation of Principal Component Analysis (PCA)\", Built In, 2023. [Online]. Available: https://builtin.com/data-science/step-step-explanation-principal-component-analysis. [Accessed: 18-Apr-2023].</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002781a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing eigen decomposition for covariance matrix\n",
    "eigenvalues, eigenvectors = LA.eigh(covMatrix)\n",
    "# Sort the eigenvectors by descending eigenvalues\n",
    "sortKey = np.argsort(eigenvalues)[::-1]\n",
    "eigenvalues = eigenvalues[sortKey]\n",
    "eigenvectors = eigenvectors[:, sortKey]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9413509b",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "###  Calculating the PCA by multiplying the Normalised Dataframe with Sorted Eigenvectors of the Covariance Matrix\n",
    "\n",
    "</br>\n",
    "\n",
    "This is done as the multiplication of **Normalized Dataframe . Eigenvectors** presents a matrix whose columns give the projections of the data points on each principal axis.\n",
    "\n",
    "**In the code cell below, Students are presented with the construction of the matrix containing the Principal axis, through the np.dot function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9407c878",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiplying the normalized Dataframe with the eigenvectors of the Covariance Matrix\n",
    "pcaSmallData2=SmalledDFMatrix@eigenvectors\n",
    "pd.DataFrame(pcaSmallData2)\n",
    "#Ordering By EigenValues\n",
    "sortIndices = np.argsort(eigenvalues)[::-1]\n",
    "pcaSmallData2 = pcaSmallData2[:,sortIndices]\n",
    "\n",
    "# Convert pcaSmallData2 to a pandas DataFrame and print it\n",
    "pd.DataFrame(pcaSmallData2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c92107",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "###  Calculating the Variance Ratio for each Principal Component in order to determine which are the best Principal Components in explaining the variation in the data\n",
    "</br>\n",
    "\n",
    "The use of variance ratios in PCA is done, in order to calculate the percentage of the overall variance in the data that each principal component contributes to. Each principal component in the PCA algorithm captures a specific amount of data variation, thus we can determine the percentage of the overall variation that each component accounts for by computing the variance ratio. Additionally, through the variance ratio we are able to determine which principal components are crucial for explaining the variation in the data. [21]</br>\n",
    "\n",
    "**Therefore, we are able to lower the number of dimensions in the data whilst preserving a sizeable portion of the overall variance, ultimately simplifying the data.**</br>\n",
    "\n",
    "\n",
    "</br><font size=\"+1\">**The Variance Ratio is calculated through the following Formula (obtained from [21-22]):**</font></br></br></br>\n",
    "\n",
    "<div style=\"text-align: center\"><font size=\"+2.4\"> $Variance Ratio = \\frac{\\lambda_i}{\\Sigma \\lambda_i}$ </font></div></br>\n",
    "\n",
    "<font size=\"+0.5\">\n",
    "    \n",
    "**where:**\n",
    "1. **$\\lambda_i$** - is the sum of the squared distance for each of the principal component\n",
    "2. **$\\Sigma \\lambda_i$** - is the sum of all squared distances for all of the principal components\n",
    "</font></br>\n",
    "\n",
    "**In the code cell below, Students are presented with the calculation of the Variance Ratio for the current PCA configuration.**\n",
    "\n",
    "\n",
    "</br></br>\n",
    "\n",
    "**Citations in this Section:** </br>\n",
    "\n",
    "[21] I. T. Jolliffe and J. Cadima, \"Principal component analysis: a review and recent developments,\" in The Data Deluge: Can Libraries Cope with E-Science? Proceedings of a Conference Held at the Royal Society, London, UK, 4-5 November 2004, vol. 463, Royal Society Publishing, 2016, pp. 21-36. doi: 10.1098/rsta.2015.0202.[Online]. Available: https://royalsocietypublishing.org/doi/10.1098/rsta.2015.0202. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[22] K. Guillaumier, \"Linear Algebra in Data Science and PCA\"</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9da2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an array to hold the square of each principal component\n",
    "square=[0]*len(pcaSmallData2[0])\n",
    "#Looping through all the principal components, and calculating the square for each component\n",
    "for row in range(len(pcaSmallData2)):\n",
    "    for col in range(len(pcaSmallData2[row])):\n",
    "        square[col]+=(pcaSmallData2[row][col])**2 \n",
    "#Calculating the total Square\n",
    "totalSquare=sum(square)\n",
    "#Calculating the Variance Ratio by dividing the square by the total square and multiplying by 100 to obtain a percentage\n",
    "covVarianceRatio=(square/totalSquare)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275739af",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "###  Visualising the Variance Ratio in the form of a Scree Plot\n",
    "</br>\n",
    "Through the use of the Scree Plot, which displays the percentage of variation explained by each primary component, we are able to calculate the number of components required to account for a specific percentage of the overall variance in the data [23].\n",
    "\n",
    "The following methods can be used to determine the optimal number of principal components to retain [23]:\n",
    "1. **Elbow Method** - This method of selection adopts to retain all the principal components prior to the curve plateau in the Scree Plot. Moreover, this method works by pinpointing the point on the Scree Plot where the curve plateaus, and then selecting the number of components before this point as the ideal number of components to maintain.\n",
    "2. **Kaiser Rule** - This method of selection selects to retain all the principal components with eigenvalues which have at least a value of 1.\n",
    "3. **Proportion of Variance Plot** - This method of selection chooses to retain all the principal components which represent a percentage (%) amount of the variance.\n",
    "\n",
    "\n",
    "\n",
    "</br></br>\n",
    "\n",
    "**Citations in this Section:** </br>\n",
    "\n",
    "[23] S. Mangale, \"Scree Plot,\" Medium, Aug. 28, 2020. [Online]. Available: https://sanchitamangale12.medium.com/scree-plot-733ed72c8608. [Accessed: 18-Apr-2023].</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8d8edd",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "###  Visual Representation of Scree Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428f9520",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a Scree Plot to visualise the importance of each Principal Component\n",
    "figScreeCov = go.Figure()\n",
    "#Creating Line Graph\n",
    "figScreeCov.add_trace(go.Scatter(y=covVarianceRatio))\n",
    "#Creating Bar Graph, with values on top rounded to 2 decimal place\n",
    "figScreeCov.add_trace(go.Bar(marker_color=covVarianceRatio,y=covVarianceRatio, text=list(np.around(np.array(covVarianceRatio),2)),textposition='outside'))\n",
    "#Updating layout\n",
    "figScreeCov.update_layout(title = 'Scree Plot of PCA via Covariance Matrix Approach of small dataset from '+filename+':',\n",
    "                    yaxis_title=\"Percentage of Explained Variance\",\n",
    "                    xaxis_title=\"Components\",\n",
    "                    font_color=\"blue\",font_family=\"verdana\", showlegend=False)\n",
    "#Displaying plot\n",
    "figScreeCov.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89526543",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "###  Visual Representation of Principal Components\n",
    "\n",
    "</br>\n",
    "\n",
    "**In the code sections below, the Best Principal Components obtained will be plotted on different axis, in order to show the correlation between the data, after the reduction in dimensions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d417575",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "###  Plotting the best three Principal Components which retain the Highest Variance in a 3D plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c13f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a 3D scatter plot, from the respective Principal Components\n",
    "fig3DCov = go.Figure(data=go.Scatter3d(\n",
    "    x=pcaSmallData2.T[0],\n",
    "    y=pcaSmallData2.T[1],\n",
    "    z=pcaSmallData2.T[2],\n",
    "    mode='markers',\n",
    "    marker=dict(color = pcaSmallData2.T[0],line=dict(width=2000, color='DarkSlateGrey'))\n",
    "))\n",
    "\n",
    "#Setting the respective title preferences\n",
    "fig3DCov.update_layout(width=1000, height=1000, title = '3D Representation PCA Data via Covariance Matrix Approach of small dataset from '+filename+':',\n",
    "                  font_color=\"blue\",font_family=\"verdana\",\n",
    "                  scene = dict(xaxis=dict(title=\"Principal Component 1\", titlefont_color='blue'),\n",
    "                               yaxis=dict(title=\"Principal Component 2\", titlefont_color='blue'),\n",
    "                               zaxis=dict(title=\"Principal Component 3\", titlefont_color='blue')))\n",
    "#Displaying plot\n",
    "fig3DCov.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26697b86",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "###  Plotting the best three Principal Components which retain the Highest Variance in a 2D plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a41fc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a 2D scatter plot, from the respective Principal Components\n",
    "fig2DCov = px.scatter(x=pcaSmallData2.T[0] ,y=pcaSmallData2.T[1])\n",
    "fig2DCov.update_layout(title = '2D Representation of PCA Data via Covariance Matrix Approach of small dataset from '+filename+':',\n",
    "                    xaxis_title=\"Principal Component 1\",\n",
    "                    yaxis_title=\"Principal Component 2\",\n",
    "                    font_color=\"blue\",font_family=\"verdana\",coloraxis_showscale=False)\n",
    "#Updating settings\n",
    "fig2DCov.update_traces(marker=dict(color=pcaSmallData2.T[0],size =10,line=dict(width=2,color='DarkSlateGrey')),selector=dict(mode='markers'))\n",
    "#Displaying plot\n",
    "fig2DCov.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9a8ace",
   "metadata": {},
   "source": [
    "</br></br>\n",
    "\n",
    "<a id='approachCompare'></a>\n",
    "### 9. Comparisons Between Approaches\n",
    "\n",
    "</br>\n",
    "Although both approaches of computing the PCA algorithm essentially provide similar results, as can be seen in following graphs below, both approaches have their fair share of differences. For instance, in the SVD approach, one can compute the principal components directly by applying SVD decomposition on the original matrix. On the other hand, in the Covariance Matrix approach one needs to first compute the Covariance matrix, and then apply Eigen Decomposition in order to compute the principal components, making the process a lengthier one when compared to the SVD approach. Moreover, the Covariance approach also tends to be quite memory inefficient, due to the construction of the Covariance matrix, since the goal of the PCA is to reduce dimensionality, whilst in this approach one must first compute a larger matrix. Performance-wise, PCA with SVD outperforms PCA with covariance and is often quicker and more numerically stable. However, in some circumstances, such as when the data includes missing values or when the data is not centred, PCA with covariance may be chosen. [20]\n",
    "\n",
    "</br>\n",
    "\n",
    "**In the code sections below, the different Scree Plots and 2D Plots for each approach are shown next to each other, so that Students can compare both approaches together visually. Moreover, there might be some discrepancies between the 2D and 3D Plots between both approaches, as the graphs would be inverted since the direction of the eigenvectors would be different.**\n",
    "\n",
    "\n",
    "</br></br>\n",
    "\n",
    "**Citations in this Section:** </br>\n",
    "\n",
    "[20] M. E. Wall, A. Rechtsteiner, and L. M. Rocha, \"Singular Value Decomposition and Principal Component Analysis,\" in Learning from Data: Concepts, Theory, and Methods, vol. 2, Springer, Boston, MA, 2007, pp. 151-176, doi: 10.1007/0-306-47815-3_5. [Online]. Available: https://www.researchgate.net/publication/2167923_Singular_Value_Decomposition_and_Principal_Component_Analysis. [Accessed: 18-Apr-2023].</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6730c779",
   "metadata": {},
   "outputs": [],
   "source": [
    "figScreeSVD.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfe94a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "figScreeCov.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2a715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2DSVD.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566c24c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2DCov.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1701f82",
   "metadata": {},
   "source": [
    "</br></br>\n",
    "\n",
    "<a id='pcaEntire'></a>\n",
    "### 10. Working out PCA on the Entire Dataset \n",
    "\n",
    "</br>\n",
    "\n",
    "The **PCA** algorithm implementations above, were implemented for the sole purpose to educate Students on how the algorithm functions. Nevertheless, utilisation of such algorithm does not require the lengthy implementation in the previous sections, as one can easily adopt to use the PCA function in the **scikit-learn library** through: **from sklearn.decomposition import PCA** import.\n",
    "</br></br>\n",
    "\n",
    "Additionally, it is interesting to know that sometimes the calculation of PCA via the NumPy library crashes the notebook when running on large datasets, whilst the PCA from the scikit-learn library does not. This is due, since in the scikit-learn library, if the input dataset has a size larger than 500 x 500, and the number of components to extract is less than 80% of the smallest dimension of the data, then a randomized SVD proposed by Halko [27] is utilised [28]. If not, the exact entire SVD is calculated and then could be truncated backwards [28]. Furthermore, utilisation of the NumPy library stores arrays in a contiguous block in memory, thus making the notebook crash in the case that the computer has insufficient memory [29]. \n",
    "\n",
    "</br>\n",
    "\n",
    "In continuation, the **Randomized SVD** proposed by Halko, can approximate the whole SVD with a substantially lower computation cost by randomly selecting a fraction of the matrix's rows or columns [27]. Thus explaining, the mystery behind the enhanced efficiency in the scikit-learn approach, when compared to the NumPy approach.\n",
    "\n",
    "</br>\n",
    "\n",
    "**In the code sections below, show the implementation of the PCA algorithm via scikit-learn library.**\n",
    "\n",
    "\n",
    "</br></br>\n",
    "\n",
    "**Citations in this Section:** </br>\n",
    "\n",
    "[27] N. Halko, P. G. Martinsson, and J. A. Tropp, “Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions,” arXiv preprint arXiv:0909.4061, 2009. [Online]. Available: https://arxiv.org/abs/0909.4061. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[28] Scikit-learn, “sklearn.decomposition.PCA\", scikit-learn.org. [Online]. Available: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[29] M. Kumar, \"Memory error in NumPy SVD,\" in IEEE, 2014. [Online]. Available: https://stackoverflow.com/questions/21180298/memory-error-in-numpy-svd. [Accessed: 18-Apr-2023].</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b229189",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling PCA algorithm from sklearn library, and feeding it the normalized Dataframe in its entirety\n",
    "pca = PCA(n_components=len(normalizedDF.columns))\n",
    "pca.fit(normalizedDF)\n",
    "pcaData=pca.transform(normalizedDF)\n",
    "#Transposing the matrix\n",
    "pcaData=pcaData.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d2a0d9",
   "metadata": {},
   "source": [
    "</br></br>\n",
    "\n",
    "<a id='pcaVisualisations'></a>\n",
    "### 11. PCA Visualisations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4472a631",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "###  Visualising the Variance Ratio in the form of a Scree Plot\n",
    "</br>\n",
    "Through the use of the Scree Plot, which displays the percentage of variation explained by each primary component, we are able to calculate the number of components required to account for a specific percentage of the overall variance in the data [23].\n",
    "\n",
    "The following methods can be used to determine the optimal number of principal components to retain [23]:\n",
    "1. **Elbow Method** - This method of selection adopts to retain all the principal components prior to the curve plateau in the Scree Plot. Moreover, this method works by pinpointing the point on the Scree Plot where the curve plateaus, and then selecting the number of components before this point as the ideal number of components to maintain.\n",
    "2. **Kaiser Rule** - This method of selection selects to retain all the principal components with eigenvalues which have at least a value of 1.\n",
    "3. **Proportion of Variance Plot** - This method of selection chooses to retain all the principal components which represent a percentage (%) amount of the variance.\n",
    "\n",
    "\n",
    "\n",
    "</br></br>\n",
    "\n",
    "**Citations in this Section:** </br>\n",
    "\n",
    "[23] S. Mangale, \"Scree Plot,\" Medium, Aug. 28, 2020. [Online]. Available: https://sanchitamangale12.medium.com/scree-plot-733ed72c8608. [Accessed: 18-Apr-2023].</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea94c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a Scree Plot to visualise the importance of each Principal Component\n",
    "figScreePca = go.Figure()\n",
    "#Creating Line Graph (multiplying variance ratio by 100 to transform into percentage)\n",
    "figScreePca.add_trace(go.Scatter(y=pca.explained_variance_ratio_*100))\n",
    "#Creating Bar Graph, with values on top rounded to 2 decimal place\n",
    "figScreePca.add_trace(go.Bar(marker_color=pca.explained_variance_ratio_*100,y=pca.explained_variance_ratio_*100, text=list(np.around(np.array(pca.explained_variance_ratio_*100),2)),textposition='outside'))\n",
    "#Updating layout\n",
    "figScreePca.update_layout(title = 'Scree Plot of PCA Algorithm on the entire dataset from '+filename+':',\n",
    "                    yaxis_title=\"Percentage of Explained Variance\",\n",
    "                    xaxis_title=\"Components\",\n",
    "                    font_color=\"blue\",font_family=\"verdana\", showlegend=False)\n",
    "#Displaying plot\n",
    "figScreePca.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e5706d",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "###  Visual Representation of Principal Components\n",
    "\n",
    "</br>\n",
    "\n",
    "**In the code sections below, the Best Principal Components obtained will be plotted on different axis, in order to show the correlation between the data, after the reduction in dimensions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c882c67e",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "###  Plotting the best three Principal Components which retain the Highest Variance in a 3D plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5068060",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a 3D scatter plot, from the respective Principal Components\n",
    "fig3DPca = go.Figure(data=go.Scatter3d(\n",
    "    x=pcaData[0],\n",
    "    y=pcaData[1],\n",
    "    z=pcaData[2],\n",
    "    mode='markers',\n",
    "    marker=dict(color = pcaData[0],line=dict(width=2000, color='DarkSlateGrey'))\n",
    "))\n",
    "\n",
    "#Setting the respective title preferences\n",
    "fig3DPca.update_layout(width=1000, height=1000, title = '3D Representation of PCA Algorithm on the entire dataset from '+filename+':',\n",
    "                  font_color=\"blue\",font_family=\"verdana\",\n",
    "                  scene = dict(xaxis=dict(title=\"Principal Component 1\", titlefont_color='blue'),\n",
    "                               yaxis=dict(title=\"Principal Component 2\", titlefont_color='blue'),\n",
    "                               zaxis=dict(title=\"Principal Component 3\", titlefont_color='blue')))\n",
    "#Displaying plot\n",
    "fig3DPca.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa9b4e1",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "###  Plotting the best two Principal Components which retain the Highest Variance in a 2D plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d41a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a 2D scatter plot, from the respective Principal Components\n",
    "fig2DPca = px.scatter(x=pcaData[0] ,y=pcaData[1])\n",
    "fig2DPca.update_layout(title = '2D Representation of PCA Algorithm on the entire dataset from '+filename+':',\n",
    "                    xaxis_title=\"Principal Component 1\",\n",
    "                    yaxis_title=\"Principal Component 2\",\n",
    "                    font_color=\"blue\",font_family=\"verdana\",coloraxis_showscale=False)\n",
    "#Updating settings\n",
    "fig2DPca.update_traces(marker=dict(color=pcaData[0],size =10,line=dict(width=2,color='DarkSlateGrey')),selector=dict(mode='markers'))\n",
    "#Displaying plot\n",
    "fig2DPca.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbae353d",
   "metadata": {},
   "source": [
    "</br>\n",
    "\n",
    "###  Plotting the best Principal Component which retain the Highest Variance in a 1D plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8d9079",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a 1D scatter plot, from the respective Principal Components\n",
    "fig1DPca = px.scatter(x=pcaData[0] ,y=[0]*len(pcaData[0]))\n",
    "fig1DPca.update_layout(title = '1D Representation of PCA Algorithm on the entire dataset from '+filename+':',\n",
    "                    xaxis_title=\"Principal Component 1\",\n",
    "                    yaxis_title=\"No Axis\",\n",
    "                    font_color=\"blue\",font_family=\"verdana\",coloraxis_showscale=False)\n",
    "#Updating settings\n",
    "fig1DPca.update_traces(marker=dict(color=pcaData[0],size =10,line=dict(width=2,color='DarkSlateGrey')),selector=dict(mode='markers'))\n",
    "#Displaying plot\n",
    "fig1DPca.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbb1d16",
   "metadata": {},
   "source": [
    "</br></br>\n",
    "\n",
    "<a id='conclusion'></a>\n",
    "### 12. Conclusions and Limitations of PCA\n",
    "\n",
    "</br>\n",
    "\n",
    "**Summary**</br>\n",
    "\n",
    "In this notebook we have explained and delved deeply in the inner workings of the PCA algorithm. Furthermore, we began by comprehending the basic principles of PCA and how it may be applied for dimensionality reduction and feature selection. Additionally, we also covered in depth, the mathematical principles of PCA and the two approaches of calculating such algorithm , such as the **Covariance Matrix approach**, **SVD approach**, eigenvectors, and eigenvalues. Moreover, through the various visualisations tools presented, we were also able to perceive and understand various data patterns and distributions. Throughout this notebook we have also discussed greatly the different types of encoding algorithms, which can be used to transform discrete data in a way which can be interpreted by the PCA algorithm.\n",
    "\n",
    "</br>\n",
    "\n",
    "**Advantages of PCA [30]:**\n",
    "1. **Easily calculable** - PCA relies on linear algebra methods which are mathematically simple to compute.\n",
    "2. **Accelerating other Machine Learning Algorithms** - These algorithms would converge faster when trained on the principal components, rather than the original dataset.\n",
    "3. **Minimising the issues of High-Dimensional Data** - Utilising the PCA algorithm to reduce the number of dimensions, would guarantee that predictive algorithms would not overfit, and thus such algorithms would learn the robust features in the data.\n",
    "\n",
    "</br>\n",
    "\n",
    "**Disadvantages of PCA [30]:**\n",
    "1. **Principal Components are poorly interpreted** - After computing the principal components, it is quite challenging to determine which features in the dataset are the most crucial. \n",
    "2. **Information loss and Dimensionality reduction Trade-off** - Whilst employing PCA, one must choose a balanced trade-off between dimensionality reduction and information loss, as dimensionality reduction comes with the cost of information loss.\n",
    "\n",
    "</br>\n",
    "\n",
    "**Assumptions taken by PCA [30]:**\n",
    "1. **PCA assumes that features are correlated** - The PCA algorithm cannot identify the principal components if the features are not correlated.\n",
    "2. **PCA assumes the connection between features is linear** - The PCA algorithm is not applicable in capturing non-linear relationships.\n",
    "\n",
    "</br>\n",
    "\n",
    "**Limitations of PCA [30]:**\n",
    "1. **The Scale of the features affects PCA** - If given unnormalised/unscaled data (some data will have a high variance, and some will have a low variance), PCA will load on the high variance data .\n",
    "2. **PCA lacks resistance to outliers** - The PCA algorithm can be biased towards outliers in the dataset. Thus, it is recommended to remove outliers beforehand.\n",
    "\n",
    "\n",
    "</br>\n",
    "\n",
    "**In this notebook, we have explored the vanilla version of the PCA algorithm, however, there are various different other types of PCA.**\n",
    "\n",
    "</br>\n",
    "\n",
    "**Types of PCA [31-32]:**\n",
    "1. **Sparse PCA** - Sparse PCA makes use of sparse loading, to attempt to generate models that are simple to understand.\n",
    "2. **Randomized PCA** - Randomized PCA makes use of randomized singular value decomposition, in order to quickly approximatively determine the first K principal components. \n",
    "3. **Incremental PCA** - Incremental PCA splits the dataset in mini-batches, and proceeds to loads each mini-batch in memory, one at a time.\n",
    "4. **Kernel PCA** - Kernel PCA is a method that projects the linearly inseparable data into a higher dimension where it is linearly separable using the so-called kernel trick. Moreover, there are various different kernels such as linear, polynomial, RBF, and sigmoid. **Furthermore, to solve one of the assumptions/limitations of PCA, i.e., PCA works only for linear datasets, the Kernel PCA can be utilised to resolve such issue.**\n",
    "5. **Robust PCA** - Robust PCA is another version of PCA which is more resilient to outliers and data errors. Additionally, Robust PCA identifies the robust principal components in the existence of data faults and outliers.\n",
    "</br>\n",
    "\n",
    "**If you are interested to learn more about the different types of PCA, you might want to look at [31] as it explains the different types of PCA, whilst showing implementations of them in python.**\n",
    "\n",
    "</br></br></br>\n",
    "\n",
    "## <div style=\"text-align: center\"> Thank you for your time and attention. Have a Good one :) </div>\n",
    "\n",
    "\n",
    "</br></br>\n",
    "\n",
    "**Citations in this Section:** </br>\n",
    "\n",
    "[30] Keboola, \"A Guide to Principal Component Analysis (PCA) for Machine Learning\", keboola.com, Apr. 02, 2022. [Online]. Available: https://www.keboola.com/blog/pca-machine-learning. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[31] N. B. Subramanian, \"Types of PCA\", aiaspirant.com. [Online]. Available: https://aiaspirant.com/types-of-pca/. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[32] E. J. Candes, X. Li, Y. Ma, and J. Wright, \"Robust Principal Component Analysis?\", Journal of the ACM (JACM), vol. 58, no. 3, 2011. [Online]. Available: https://arxiv.org/pdf/0912.3599.pdf. [Accessed: 18-Apr-2023].</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37ff494",
   "metadata": {},
   "source": [
    "</br></br>\n",
    "\n",
    "<a id='references'></a>\n",
    "### 13. References\n",
    "\n",
    "</br>\n",
    "\n",
    "[1] S. Mishra et al., \"Multivariate Statistical Data Analysis-Principal Component Analysis,\" Int. J. Livest. Res., vol. 1, pp. 1-6, 2017, [Online]. Available: https://www.researchgate.net/publication/316652806_Principal_Component_Analysis. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "\n",
    "[2] D. Li and S. Liu, \"4.2.3.1 Principal Component Analysis,\" in Water Quality Monitoring and Management: Basis, Technology and Case Studies, 1st ed., S. K. Gupta and R. Kumar, Eds. Amsterdam, Netherlands: Elsevier, 2019, [Online]. Available: https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/principal-component-analysis. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[3] DEVAKUMAR K. P., \"COVID-19 Dataset\", Kaggle, 2020. [Online]. Available: https://www.kaggle.com/datasets/imdevskp/corona-virus-report?select=country_wise_latest.csv. [Accessed: 18-Apr-2023].</br>\n",
    " \n",
    "[4] UCI MACHINE LEARNING, \"Pima Indians Diabetes Database\", Kaggle, 2016. [Online]. Available: https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[5] S. BANERJEE, \"FIFA - Football World Cup Dataset\", Kaggle, 2022. [Online]. Available: https://www.kaggle.com/datasets/iamsouravbanerjee/fifa-football-world-cup-dataset?select=FIFA+-+2014.csv. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[6] MATHNERD, \"Iris Flower Dataset\", Kaggle, 2018. [Online]. Available: https://www.kaggle.com/datasets/arshid/iris-flower-dataset. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[7] S. BANERJEE, \"Software Industry Salary Dataset - 2022\", Kaggle, 2022. [Online]. Available: https://www.kaggle.com/datasets/iamsouravbanerjee/software-professional-salaries-2022. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[8] R. Holbrook and A. Cook, \"Principal Component Analysis, spotify.csv\", Kaggle. [Online]. Available: https://www.kaggle.com/code/ryanholbrook/principal-component-analysis/data?select=spotify.csv. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[9] RUTHGN, \"Wine Quality Data Set (Red & White Wine)\", Kaggle, 2022. [Online]. Available: https://www.kaggle.com/datasets/ruthgn/wine-quality-data-set-red-white-wine. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[10] V. Karthik, \"PCA for categorical features\", Stack Overflow, Dec. 2016. [Online]. Available: https://stackoverflow.com/questions/40795141/pca-for-categorical-features#:~:text=PCA%20is%20designed%20for%20continuous,yes%2C%20you%20can%20use%20PCA. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[11] Datagy. \"Pandas get_dummies (One-Hot Encoding) Explained,\" Datagy.io, Feb. 2021. [Online]. Available: https://datagy.io/pandas-get-dummies/. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[12] DataCamp. \"Dealing with Categorical Data\". DataCamp, 2021. [Online]. Available: https://www.datacamp.com/tutorial/categorical-data. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[13] B. Roy, \"All about Categorical Variable Encoding,\" Towards Data Science, Jul. 2, 2019. [Online]. Available: https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[14] T. Crosley, \"What is the binary to decimal decoder?\", Quora, May 8, 2018. [Online]. Available: https://www.quora.com/What-is-the-binary-to-decimal-decoder. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[15] Pandas. \"pandas.factorize()\". pandas 1.4.0 documentation, Jan. 07, 2022. [Online]. Available: https://pandas.pydata.org/docs/reference/api/pandas.factorize.html. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[16] J. Brownlee, \"One-Hot Encoding for Categorical Data,\" Machine Learning Mastery, Aug. 17, 2020. [Online]. Available: https://machinelearningmastery.com/one-hot-encoding-for-categorical-data/. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[17] Vatsal, \"Word2Vec Explained\", Towards Data Science, Jul. 29, 2021. [Online]. Available: https://towardsdatascience.com/word2vec-explained-49c52b4ccb71. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[18] Stack Exchange. \"Why do we need to normalize data before Principal Component Analysis (PCA)?\", Cross Validated, May 26, 2014. [Online]. Available: https://stats.stackexchange.com/questions/69157/why-do-we-need-to-normalize-data-before-principal-component-analysis-pca. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[19] R. Sharma. \"What is Normalization in Data Mining and How to Do It?\", UpGrad, Sep. 22, 2022. [Online]. Available: https://www.upgrad.com/blog/normalization-in-data-mining/#:~:text=Project%20Ideas%20%26%20Topics-,Z%2DScore%20Normalization,up%20to%20%2B3%20standard%20deviation. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[20] M. E. Wall, A. Rechtsteiner, and L. M. Rocha, \"Singular Value Decomposition and Principal Component Analysis,\" in Learning from Data: Concepts, Theory, and Methods, vol. 2, Springer, Boston, MA, 2007, pp. 151-176, doi: 10.1007/0-306-47815-3_5. [Online]. Available: https://www.researchgate.net/publication/2167923_Singular_Value_Decomposition_and_Principal_Component_Analysis. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[21] I. T. Jolliffe and J. Cadima, \"Principal component analysis: a review and recent developments,\" in The Data Deluge: Can Libraries Cope with E-Science? Proceedings of a Conference Held at the Royal Society, London, UK, 4-5 November 2004, vol. 463, Royal Society Publishing, 2016, pp. 21-36. doi: 10.1098/rsta.2015.0202.[Online]. Available: https://royalsocietypublishing.org/doi/10.1098/rsta.2015.0202. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[22] K. Guillaumier, \"Linear Algebra in Data Science and PCA\"</br>\n",
    "\n",
    "[23] S. Mangale, \"Scree Plot,\" Medium, Aug. 28, 2020. [Online]. Available: https://sanchitamangale12.medium.com/scree-plot-733ed72c8608. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[24] CUEMATH, \"Covariance Matrix\", CUEMATH. [Online]. Available: https://www.cuemath.com/algebra/covariance-matrix/. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[25] Minitab LLC, \"Interpret the key results for Covariance\", Minitab Support, 2022. [Online]. Available: https://support.minitab.com/en-us/minitab/21/help-and-how-to/statistics/basic-statistics/how-to/covariance/interpret-the-results/key-results/. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[26] Z. Jaadi, \"A Step-by-Step Explanation of Principal Component Analysis (PCA)\", Built In, 2023. [Online]. Available: https://builtin.com/data-science/step-step-explanation-principal-component-analysis. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[27] N. Halko, P. G. Martinsson, and J. A. Tropp, “Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions,” arXiv preprint arXiv:0909.4061, 2009. [Online]. Available: https://arxiv.org/abs/0909.4061. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[28] Scikit-learn, “sklearn.decomposition.PCA\", scikit-learn.org. [Online]. Available: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[29] M. Kumar, \"Memory error in NumPy SVD,\" in IEEE, 2014. [Online]. Available: https://stackoverflow.com/questions/21180298/memory-error-in-numpy-svd. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[30] Keboola, \"A Guide to Principal Component Analysis (PCA) for Machine Learning\", keboola.com, Apr. 02, 2022. [Online]. Available: https://www.keboola.com/blog/pca-machine-learning. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[31] N. B. Subramanian, \"Types of PCA\", aiaspirant.com. [Online]. Available: https://aiaspirant.com/types-of-pca/. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "[32] E. J. Candes, X. Li, Y. Ma, and J. Wright, \"Robust Principal Component Analysis?\", Journal of the ACM (JACM), vol. 58, no. 3, 2011. [Online]. Available: https://arxiv.org/pdf/0912.3599.pdf. [Accessed: 18-Apr-2023].</br>\n",
    "\n",
    "</br></br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0e14e4",
   "metadata": {},
   "source": [
    "</br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
